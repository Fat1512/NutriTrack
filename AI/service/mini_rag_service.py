from typing import List, Dict, Any
from datetime import datetime, timezone, timedelta
from dateutil import parser as date_parser

from components.manager import EmbeddingManager
from langchain_text_splitters import RecursiveCharacterTextSplitter
from components.manager import ReaderManager
from components.manager import DatabaseManager

class MiniRagService:
    def __init__(self, collection_name="rag_documents"):
        self.embedder = EmbeddingManager()
        self.reader_manager = ReaderManager()
        self.db = DatabaseManager()
        self.collection_name = collection_name

        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        print("MiniRag initialized.")

    def ingest_file(self, file_path: str, filename: str) -> Dict[str, Any]:
        text, error = self.reader_manager.read_file(file_path)
        if error:
            return {"error": error, "filename": filename}
        
        return self._process_and_store_text(text, filename, extra_metadata=None)

    def ingest_bytes(self, raw_bytes: bytes, filename: str, extra_metadata: dict = None) -> Dict[str, Any]:
        try:
            text = raw_bytes.decode('utf-8')
        except Exception as e:
            return {"error": f"Error decoding bytes: {e}", "filename": filename}
        
        return self._process_and_store_text(text, filename, extra_metadata)

    def _process_and_store_text(self, text: str, filename: str, extra_metadata: dict = None) -> Dict[str, Any]:
        chunks = self.text_splitter.split_text(text)
        print(f"Split '{filename}' into {len(chunks)} chunks.")

        embeddings = self.embedder.vectorize(chunks)

        ids = [f"{filename}_{i}" for i in range(len(chunks))]
        metadatas = []
        for i in range(len(chunks)):
            meta = {"source": filename, "chunk_index": i}
            if extra_metadata:
                meta.update(extra_metadata)
            metadatas.append(meta)
        
        documents = chunks
        
        self.db.delete(
            collection_name=self.collection_name,
            where_filter={"source": filename}
        )
        
        self.db.add(
            collection_name=self.collection_name,
            embeddings=embeddings,
            documents=documents,
            metadatas=metadatas,
            ids=ids
        )
        
        print(f"Successfully ingested and vectorized file: {filename}")
        return {
            "status": "success",
            "filename": filename,
            "chunks_added": len(chunks)
        }

    def retrieve_context(self, query: str, n_results: int = 3) -> List[str]:
        query_vector = self.embedder.vectorize_single(query)
        results = self.db.query(
            collection_name=self.collection_name,
            query_embeddings=[query_vector],
            n_results=n_results
        )
        
        return results.get("documents", [[]])[0]

    def list_documents(self) -> List[str]:
        print("Listing unique documents...")
        unique_files = self.db.get_unique_metadata_values(
            self.collection_name, 
            "source"
        )
        return sorted(list(unique_files))

    def delete_document(self, filename: str) -> Dict[str, Any]:
        print(f"Attempting to delete document: {filename}")
        try:
            self.db.delete(
                collection_name=self.collection_name,
                where_filter={"source": filename}
            )
            print(f"Successfully deleted document: {filename}")
            return {"status": "deleted", "filename": filename}
        except Exception as e:
            print(f"Error deleting document {filename}: {e}")
            return {"error": str(e), "filename": filename}

    def document_exists(self, filename: str) -> bool:
        results = self.db.get_unique_metadata_values(self.collection_name, "source")
        return filename in results

    def delete_documents_older_than(self, prefix: str, days: int):
        if days <= 0:
            print(f"Stale document deletion is disabled (days={days}).")
            return 0
        
        if date_parser is None:
            print("Cannot delete old files: 'python-dateutil' is not installed.")
            return 0

        try:
            cutoff_dt = datetime.now(timezone.utc) - timedelta(days=days)
            print(f"Deleting documents with prefix '{prefix}' published before {cutoff_dt.isoformat()}...")

            # --- Báº®T Äáº¦U LOGIC Má»šI ---
            
            # 1. Láº¥y Táº¤T Cáº¢ metadata tá»« DB báº±ng phÆ°Æ¡ng thá»©c má»›i
            db_data = self.db.get_all(
                collection_name=self.collection_name,
                include=["metadatas"]
            )

            all_metadatas = db_data.get('metadatas', [])
            if not all_metadatas:
                print("No documents found in the collection. Nothing to check.")
                return 0

            # 2. XÃ¢y dá»±ng má»™t dict Ä‘á»ƒ theo dÃµi ngÃ y cÅ© nháº¥t cá»§a má»—i file
            #    vÃ  lá»c cÃ¡c file cÃ³ prefix
            #    Format: { "filename": "oldest_date_found" }
            document_dates = {}
            
            for meta in all_metadatas:
                filename = meta.get("source")
                pub_date_str = meta.get("publication_date") # Láº¥y ngÃ y xuáº¥t báº£n tá»« metadata

                if not filename or not pub_date_str:
                    # Bá» qua chunk nÃ y náº¿u nÃ³ khÃ´ng cÃ³ 'source' hoáº·c 'publication_date'
                    continue
                
                # Chá»‰ quan tÃ¢m Ä‘áº¿n cÃ¡c file cÃ³ prefix
                if not filename.startswith(prefix):
                    continue
                    
                # Parse ngÃ y
                try:
                    pub_date_dt = date_parser.parse(pub_date_str)
                    if pub_date_dt.tzinfo is None:
                        pub_date_dt = pub_date_dt.replace(tzinfo=timezone.utc)
                except Exception as e:
                    print(f"Skipping check for {filename}: Cound not parse date '{pub_date_str}'. Error: {e}")
                    continue

                # VÃ¬ táº¥t cáº£ cÃ¡c chunk cá»§a má»™t file Ä‘á»u cÃ³ cÃ¹ng 1 ngÃ y, 
                # chÃºng ta chá»‰ cáº§n lÆ°u láº¡i ngÃ y Ä‘áº§u tiÃªn tÃ¬m tháº¥y.
                if filename not in document_dates:
                    document_dates[filename] = pub_date_dt

            # 3. Láº·p qua danh sÃ¡ch Ä‘Ã£ lá»c vÃ  xÃ³a náº¿u quÃ¡ cÅ©
            if not document_dates:
                print(f"No documents with prefix '{prefix}' and valid 'publication_date' found.")
                return 0

            print(f"Found {len(document_dates)} document(s) with prefix '{prefix}'. Checking publication dates...")

            deleted_count = 0
            for filename, pub_date in document_dates.items():
                if pub_date < cutoff_dt:
                    print(f"ðŸ—‘ï¸ Deleting stale file: {filename} (Published: {pub_date.isoformat()})")
                    # Gá»i hÃ m delete_document hiá»‡n cÃ³ (sáº½ xÃ³a táº¥t cáº£ cÃ¡c chunk cá»§a file)
                    self.delete_document(filename)
                    deleted_count += 1

            # --- Káº¾T THÃšC LOGIC Má»šI ---

            print(f"Stale document check complete. Deleted {deleted_count} document(s).")
            return deleted_count

        except Exception as e:
            print(f"Error in delete_documents_older_than: {e}")
            return 0